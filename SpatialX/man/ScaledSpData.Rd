% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spatialtf.R
\name{ScaledSpData.default}
\alias{ScaledSpData.default}
\alias{ScaledSpData}
\title{Scale Spatial Transcriptomics Data}
\usage{
\method{ScaledSpData}{default}(
  object,
  features = NULL,
  scale.method = "standard",
  do.scale = TRUE,
  do.center = TRUE,
  scale.max = 10,
  block.size = 1000,
  min.cells.to.block = 3000,
  verbose = TRUE,
  ...
)

ScaledSpData(object, ...)
}
\arguments{
\item{object}{A matrix-like object containing normalized expression data where 
rows represent features (genes) and columns represent cells/spots.}

\item{features}{Vector of feature names to scale/center. Default is all features 
in the object.}

\item{scale.method}{Method for scaling. Available options:
\itemize{
  \item{"standard"}{ - Standard scaling (Z-score normalization)}
  \item{"robust"}{ - Robust scaling using median and MAD}
}}

\item{do.scale}{Whether to scale the data (default: TRUE).}

\item{do.center}{Whether to center the data (default: TRUE).}

\item{scale.max}{Maximum absolute value to return for scaled data. Values beyond 
this threshold will be clipped. The default is 10. Setting this can help reduce 
the effects of extreme outliers.}

\item{block.size}{Number of features to process in a single computation block. 
Increasing block.size may speed up calculations but at an additional memory cost 
(default: 1000).}

\item{min.cells.to.block}{If object contains fewer than this number of cells, 
don't use blocking for scaling calculations (default: 3000).}

\item{verbose}{Whether to display progress bar for scaling procedure (default: TRUE).}

\item{...}{Additional arguments passed to scaling methods.}
}
\value{
A scaled matrix of the same dimensions as the input object, with 
centered and/or scaled expression values.
}
\description{
This function centers and scales the expression data to standardize feature 
distributions, making them comparable across cells/spots. It supports both 
standard scaling and robust scaling methods, with efficient parallel processing 
for large datasets.
}
\details{
The function provides two scaling approaches:
\itemize{
  \item{\strong{standard scaling}:} Centers the data (subtract mean) and scales 
  (divide by standard deviation) to produce Z-scores.
  \item{\strong{robust scaling}:} Centers using median and scales using median 
  absolute deviation (MAD), making it more robust to outliers.
}

For large datasets, the function automatically uses parallel processing when 
multiple workers are available. Data is processed in blocks to manage memory 
usage efficiently.
}
\section{Parallel Processing}{

When multiple workers are available (detected via \code{future::nbrOfWorkers()}),
the function processes data in parallel using \code{future.apply::future_lapply}.
The data is split into blocks of features for efficient computation.
}

\section{Memory Management}{

For sparse matrices (dgCMatrix, dgTMatrix), the function uses optimized sparse 
matrix operations. For dense matrices, it converts to standard matrix format 
and uses efficient row-wise operations.
}

\examples{
\dontrun{# Create example normalized spatial data
data <- matrix(rnorm(1000), nrow = 100, ncol = 10)
rownames(data) <- paste0("Gene", 1:100)
colnames(data) <- paste0("Spot", 1:10)

# Perform standard scaling (center and scale)
scaled_data <- ScaledSpData(object = data, 
                           scale.method = "standard",
                           do.scale = TRUE,
                           do.center = TRUE)

# Perform only centering (no scaling)
centered_data <- ScaledSpData(object = data,
                             do.scale = FALSE,
                             do.center = TRUE)

# Perform robust scaling
robust_scaled_data <- ScaledSpData(object = data,
                                  scale.method = "robust")

# Scale only specific features
variable_genes <- paste0("Gene", 1:50)
subset_scaled <- ScaledSpData(object = data,
                             features = variable_genes)
}
}
\concept{preprocessing}
\concept{scaling}
